{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZ-bgjofaOh"
      },
      "source": [
        "#  **Pneumonia identification from X-Ray images**\n",
        "\n",
        "Group:\n",
        "\n",
        "Evaluation criteria:\n",
        "* 5 points for the delivery of a meaningful Spark-based solution\n",
        "* 2 points for the quality of the results obtained (using BigDL means a minimum of 1 point in this section).\n",
        "* 2 points for style / code cleanliness / documentation\n",
        "* 2 points for cross-evaluation of all members of the group among yourselves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTmvLgNwgdUi"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upfstbM1gfT7"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBWVU_bhfkY7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voMBntim9bMf"
      },
      "source": [
        "### **Environment Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_OS4HKJMNpv"
      },
      "source": [
        "**Install bigdl-dllib**\n",
        "\n",
        "You can install the latest pre-release version using `pip install --pre --upgrade bigdl-dllib`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qfT8CaC51hI"
      },
      "outputs": [],
      "source": [
        "# Install latest pre-release version of bigdl-dllib with spark3\n",
        "# Find the latest bigdl-dllib with spark3 from https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/ and intall it\n",
        "#!pip install https://sourceforge.net/projects/analytics-zoo/files/dllib-py-spark3/bigdl_dllib_spark3-0.14.0b20211107-py3-none-manylinux1_x86_64.whl\n",
        "\n",
        "#exit() # restart the runtime to refresh installed pkg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bigdl-spark3\n",
        "exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCxdil5JDkq0",
        "outputId": "5ee1aae1-bf03-4127-a265-dc4386e414d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bigdl-spark3 in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
            "Requirement already satisfied: bigdl-chronos-spark3==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: bigdl-orca-spark3==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: bigdl-friesian-spark3==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: bigdl-serving==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: bigdl-nano==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: pandas<=1.3.5,>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from bigdl-chronos-spark3==2.2.0->bigdl-spark3) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn<=1.0.2,>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-chronos-spark3==2.2.0->bigdl-spark3) (1.0.2)\n",
            "Requirement already satisfied: numpy<=1.23.5 in /usr/local/lib/python3.8/dist-packages (from bigdl-chronos-spark3==2.2.0->bigdl-spark3) (1.21.6)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.8/dist-packages (from bigdl-nano==2.2.0->bigdl-spark3) (2023.0.0)\n",
            "Requirement already satisfied: protobuf==3.19.5 in /usr/local/lib/python3.8/dist-packages (from bigdl-nano==2.2.0->bigdl-spark3) (3.19.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from bigdl-nano==2.2.0->bigdl-spark3) (2.2.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.8/dist-packages (from bigdl-nano==2.2.0->bigdl-spark3) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from bigdl-orca-spark3==2.2.0->bigdl-spark3) (3.9.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.8/dist-packages (from bigdl-orca-spark3==2.2.0->bigdl-spark3) (23.2.1)\n",
            "Requirement already satisfied: bigdl-dllib-spark3==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-orca-spark3==2.2.0->bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: bigdl-math==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-orca-spark3==2.2.0->bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from bigdl-orca-spark3==2.2.0->bigdl-spark3) (23.0)\n",
            "Requirement already satisfied: bigdl-tf==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-orca-spark3==2.2.0->bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from bigdl-serving==2.2.0->bigdl-spark3) (6.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from bigdl-serving==2.2.0->bigdl-spark3) (9.0.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.8/dist-packages (from bigdl-serving==2.2.0->bigdl-spark3) (0.23.3)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.8/dist-packages (from bigdl-serving==2.2.0->bigdl-spark3) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from bigdl-serving==2.2.0->bigdl-spark3) (4.6.0.66)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.8/dist-packages (from bigdl-serving==2.2.0->bigdl-spark3) (4.5.0)\n",
            "Requirement already satisfied: conda-pack==0.3.1 in /usr/local/lib/python3.8/dist-packages (from bigdl-dllib-spark3==2.2.0->bigdl-orca-spark3==2.2.0->bigdl-spark3) (0.3.1)\n",
            "Requirement already satisfied: pyspark==3.1.3 in /usr/local/lib/python3.8/dist-packages (from bigdl-dllib-spark3==2.2.0->bigdl-orca-spark3==2.2.0->bigdl-spark3) (3.1.3)\n",
            "Requirement already satisfied: bigdl-core==2.2.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-dllib-spark3==2.2.0->bigdl-orca-spark3==2.2.0->bigdl-spark3) (2.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from bigdl-dllib-spark3==2.2.0->bigdl-orca-spark3==2.2.0->bigdl-spark3) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from conda-pack==0.3.1->bigdl-dllib-spark3==2.2.0->bigdl-orca-spark3==2.2.0->bigdl-spark3) (57.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.8/dist-packages (from pyspark==3.1.3->bigdl-dllib-spark3==2.2.0->bigdl-orca-spark3==2.2.0->bigdl-spark3) (0.10.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<=1.3.5,>=1.0.5->bigdl-chronos-spark3==2.2.0->bigdl-spark3) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas<=1.3.5,>=1.0.5->bigdl-chronos-spark3==2.2.0->bigdl-spark3) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<=1.0.2,>=0.22.0->bigdl-chronos-spark3==2.2.0->bigdl-spark3) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<=1.0.2,>=0.22.0->bigdl-chronos-spark3==2.2.0->bigdl-spark3) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<=1.0.2,>=0.22.0->bigdl-chronos-spark3==2.2.0->bigdl-spark3) (3.1.0)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.8/dist-packages (from httpx->bigdl-serving==2.2.0->bigdl-spark3) (1.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.8/dist-packages (from httpx->bigdl-serving==2.2.0->bigdl-spark3) (1.3.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from httpx->bigdl-serving==2.2.0->bigdl-spark3) (0.16.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->bigdl-serving==2.2.0->bigdl-spark3) (2022.12.7)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from redis->bigdl-serving==2.2.0->bigdl-spark3) (4.0.2)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.8/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx->bigdl-serving==2.2.0->bigdl-spark3) (3.6.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx->bigdl-serving==2.2.0->bigdl-spark3) (0.14.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from rfc3986[idna2008]<2,>=1.3->httpx->bigdl-serving==2.2.0->bigdl-spark3) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84cTfA0b8xL6"
      },
      "source": [
        "#### **Step 0: Intialization of pyspark and bigdl** \n",
        "\n",
        "First we import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4ukzsLiu886t"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from bigdl.dllib.nncontext import *\n",
        "from bigdl.dllib.keras.layers import *\n",
        "from bigdl.dllib.keras.models import *\n",
        "import bigdl.dllib.keras.Sequential\n",
        "from bigdl.dllib.nnframes import *\n",
        "from bigdl.dllib.nn.criterion import *\n",
        "from bigdl.dllib.feature.image import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i_mMkHEKLyk"
      },
      "source": [
        "Init NNContext and create Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LKXOL7yP9Oeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96da4711-75a9-4c87-ffff-336b99d00555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current pyspark location is : /usr/local/lib/python3.8/dist-packages/pyspark/__init__.py\n",
            "Start to getOrCreate SparkContext\n",
            "pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.8/dist-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_3.1.3-2.2.0-jar-with-dependencies.jar:/usr/local/lib/python3.8/dist-packages/bigdl/share/core/lib/all-2.2.0.jar:/usr/local/lib/python3.8/dist-packages/bigdl/share/orca/lib/bigdl-orca-spark_3.1.3-2.2.0-jar-with-dependencies.jar:/usr/local/lib/python3.8/dist-packages/bigdl/share/friesian/lib/bigdl-friesian-spark_3.1.3-2.2.0-jar-with-dependencies.jar pyspark-shell \n",
            "Successfully got a SparkContext\n"
          ]
        }
      ],
      "source": [
        "sc = init_nncontext(cluster_mode=\"local\") # run in local mode\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsT-0y8w-6N5"
      },
      "source": [
        "#### **Step 1: Load the data** \n",
        "\n",
        "Documentation\n",
        "\n",
        "https://bigdl.readthedocs.io/en/latest/doc/DLlib/QuickStart/python-getting-started.html?highlight=NNImageReader#distributed-model-training\n",
        "\n",
        "https://bigdl.readthedocs.io/en/latest/doc/DLlib/Overview/nnframes.html?highlight=NNImageReader#nnimagereader\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1gmNzXYJ8cq",
        "outputId": "2115c607-d536-4149-b4c3-4ed86bf6e617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import images with NNImageReader of BigDL from our drive."
      ],
      "metadata": {
        "id": "JdnaZxytw55o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LVpFKkCX_3WF"
      },
      "outputs": [],
      "source": [
        "normal_train = NNImageReader.readImages(\"/content/drive/MyDrive/chest_xray/train/NORMAL/\", sc)\n",
        "normal_test = NNImageReader.readImages(\"/content/drive/MyDrive/chest_xray/test/NORMAL/\", sc)\n",
        "normal_val = NNImageReader.readImages(\"/content/drive/MyDrive/chest_xray/val/NORMAL/\", sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "k6jjrQ3mpnjZ"
      },
      "outputs": [],
      "source": [
        "pneumonia_train = NNImageReader.readImages(\"/content/drive/MyDrive/chest_xray/train/PNEUMONIA/\", sc)\n",
        "pneumonia_test = NNImageReader.readImages(\"/content/drive/MyDrive/chest_xray/test/PNEUMONIA/\", sc)\n",
        "pneumonia_val = NNImageReader.readImages(\"/content/drive/MyDrive/chest_xray/val/PNEUMONIA/\", sc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2rUzrzfAL2H",
        "outputId": "f4d92d14-85ef-4ba5-ef0a-7e20a545270d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[image: struct<origin:string,height:int,width:int,nChannels:int,mode:int,data:binary>]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The data schema\n",
        "normal_train.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VltB-E05vN6S",
        "outputId": "b6cad0fe-1c0f-4d81-9aab-bc3b21033171"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- image: struct (nullable = true)\n",
            " |    |-- origin: string (nullable = true)\n",
            " |    |-- height: integer (nullable = false)\n",
            " |    |-- width: integer (nullable = false)\n",
            " |    |-- nChannels: integer (nullable = false)\n",
            " |    |-- mode: integer (nullable = false)\n",
            " |    |-- data: binary (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAIDjj4XnMzx"
      },
      "source": [
        "#### **Step 2: Distributed Data Loading**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the function *.lit* to label the images, 0=normal and 1=pneumonia.\n"
      ],
      "metadata": {
        "id": "LJkQcl-ewVgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,lit\n",
        "\n",
        "label_normal_train = normal_train.select(col(\"image\"),lit(\"0\").alias(\"label\"))\n",
        "label_normal_test = normal_test.select(col(\"image\"),lit(\"0\").alias(\"label\"))\n",
        "label_normal_val = normal_val.select(col(\"image\"),lit(\"0\").alias(\"label\"))\n",
        "\n",
        "label_pneumonia_train = pneumonia_train.select(col(\"image\"),lit(\"1\").alias(\"label\"))\n",
        "label_pneumonia_test = pneumonia_test.select(col(\"image\"),lit(\"1\").alias(\"label\"))\n",
        "label_pneumonia_val = pneumonia_val.select(col(\"image\"),lit(\"1\").alias(\"label\"))"
      ],
      "metadata": {
        "id": "GZv4eXLYzTmh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_normal_train.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd-sViVSLWHP",
        "outputId": "2e0ef11f-e834-42af-ccda-5d3802f46c0a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- image: struct (nullable = true)\n",
            " |    |-- origin: string (nullable = true)\n",
            " |    |-- height: integer (nullable = false)\n",
            " |    |-- width: integer (nullable = false)\n",
            " |    |-- nChannels: integer (nullable = false)\n",
            " |    |-- mode: integer (nullable = false)\n",
            " |    |-- data: binary (nullable = false)\n",
            " |-- label: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then with .union we leave in the same dataframe all the images both normal and with pneumonia, this for test, validation and training.\n"
      ],
      "metadata": {
        "id": "TSSAEokewZrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ = label_normal_train.union(label_pneumonia_train)\n",
        "test_ = label_normal_test.union(label_pneumonia_test)\n",
        "val_ = label_normal_val.union(label_pneumonia_val)"
      ],
      "metadata": {
        "id": "uulk3BCZwvoC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the schema of the \"train_\" dataframe.\n",
        "train_.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXFw4xaHMQlZ",
        "outputId": "1902d1af-ea59-4513-8598-417d5c3aff1d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- image: struct (nullable = true)\n",
            " |    |-- origin: string (nullable = true)\n",
            " |    |-- height: integer (nullable = false)\n",
            " |    |-- width: integer (nullable = false)\n",
            " |    |-- nChannels: integer (nullable = false)\n",
            " |    |-- mode: integer (nullable = false)\n",
            " |    |-- data: binary (nullable = false)\n",
            " |-- label: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRTHv7a2nxQy"
      },
      "source": [
        "#### **Step 3: Model Definition**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use input to determine the first layer of the model and for the remaining layers, the input dimension will be automatically inferred.\n",
        "And we use the \"relu\" layer which replaces all negative values received at the input with zeros. The interest of these activation layers is to make the model nonlinear and therefore more complex."
      ],
      "metadata": {
        "id": "zOnUQgpLv290"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wtyY5SKkoIhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57dcbd2-6eab-48d1-be85-e1e509a1f644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createZooKerasInput\n",
            "creating: createZooKerasDense\n",
            "creating: createZooKerasDense\n",
            "creating: createZooKerasDense\n",
            "creating: createZooKerasModel\n"
          ]
        }
      ],
      "source": [
        "x1 = Input(shape=[8])\n",
        "dense1 = Dense(12, activation=\"relu\")(x1)\n",
        "dense2 = Dense(8, activation=\"relu\")(dense1)\n",
        "dense3 = Dense(2)(dense2)\n",
        "model = Model(input=x1, output=dense3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use the compile function of the model to set the loss function and optimization method."
      ],
      "metadata": {
        "id": "pdIJu68QwIXD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zgjFeN3pok4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc6d93e-d542-4245-b5a6-1fddd23f853a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createAdam\n",
            "creating: createZooKerasSparseCategoricalCrossEntropy\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_paWC1hInxO7"
      },
      "source": [
        "#### **Step 4: Distributed Model Training**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Use BigDL Library to perform deep learning on image data.\n",
        "We consider resizing the image to 50x50 using the \"ImageResize\" class and mirrored the image using the \"ImageMirrror\" class.\n",
        "We have got an error due to the values in ImageResize, batch_size and nb_epoch. We have tried several options but we did not succeed."
      ],
      "metadata": {
        "id": "I3O7wfQW7r58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "OVdkMmdHoILA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359bd544-ef7d-4cac-964e-8a73f3022d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating: createImageResize\n",
            "creating: createImageMirror\n",
            "creating: createCompose\n"
          ]
        }
      ],
      "source": [
        "from bigdl.dllib.feature.image import transforms\n",
        "transformers = transforms.Compose([ImageResize(50, 50), ImageMirror()])\n",
        "#model.fit(train_, label_cols=[\"label\"], batch_size=1, nb_epoch=1, transform=transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbBcwFAxnxMj"
      },
      "source": [
        "#### **Step 5: Model saving and loading**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLBx7m1poH13"
      },
      "outputs": [],
      "source": [
        "# save\n",
        "modelPath = \"/tmp/demo/keras.model\"\n",
        "dmodel.saveModel(modelPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XG0f6x1ouGk"
      },
      "outputs": [],
      "source": [
        "# load\n",
        "loadModel = Model.loadModel(modelPath)\n",
        "preDF = loadModel.predict(df, feature_cols=[\"features\"], prediction_col=\"predict\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQN1lW4enxKN"
      },
      "source": [
        "#### **Step 6: Distributed evaluation and inference**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCPdK2tToHT-"
      },
      "outputs": [],
      "source": [
        "# inference\n",
        "model.predict(df, prediction_col=\"predict\", transform=transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C1bel4Ko3dl"
      },
      "outputs": [],
      "source": [
        "# evaluation\n",
        "model.evaluate(image_df, batch_size=1, label_cols=[\"label\"], transform=transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJA20ge3nxHm"
      },
      "source": [
        "#### **Step 7: Checkpointing and resuming training**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf5B6dCnoHBT"
      },
      "outputs": [],
      "source": [
        "cpPath = \"/tmp/demo/cp\"\n",
        "dmodel.set_checkpoint(cpPath)\n",
        "\n",
        "loadModel = Model.loadModel(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxA7vb3HnxFD"
      },
      "source": [
        "#### **Step 8: Monitor your training**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCCr50M3oGtN"
      },
      "outputs": [],
      "source": [
        "dmodel.set_tensorboard(\"./\", \"dllib_demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdQGvW_4nxCt"
      },
      "source": [
        "#### **Step 9: Transfer learning and finetuning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL1gPCRRoGaR"
      },
      "outputs": [],
      "source": [
        "dmodel.freeze(layer_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_nBY3unpICZ"
      },
      "outputs": [],
      "source": [
        "dmodel.unFreeze(layer_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF4vYp5QnxAd"
      },
      "source": [
        "#### **Step 10: Hyperparameter tuning**\n",
        "\n",
        "* optimizer\n",
        "\n",
        "DLLib supports a list of optimization methods. For more details, please refer optimization\n",
        "\n",
        "* learning rate scheduler\n",
        "\n",
        "DLLib supports a list of learning rate scheduler. For more details, please refer lr_scheduler\n",
        "\n",
        "* batch size\n",
        "\n",
        "DLLib supports set batch size during training and prediction. We can adjust the batch size to tune the modelâ€™s accuracy.\n",
        "\n",
        "* regularizer\n",
        "\n",
        "DLLib supports a list of regularizers. For more details, please refer regularizer\n",
        "\n",
        "* clipping\n",
        "\n",
        "DLLib supports gradient clipping operations. For more details, please refer gradient_clip"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}